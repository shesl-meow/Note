# 降低损失

## 迭代方法

下图显示了机器学习算法用于训练模型的迭代试错过程：

![IterativeApproach](./IterativeApproach.svg)

1. `模型`部分将一个或多个特征作为输入，然后返回一个预测 (y') 作为输出。

2. `计算损失`部分是模型将要使用的[损失函数](https://developers.google.cn/machine-learning/crash-course/descending-into-ml/training-and-loss)。

3. `计算参数更新`部分：机器学习系统就是在此部分检查损失函数的值，并生成新参数值。机器学习系统将根据所有标签重新评估所有特征，为损失函数生成一个新值，而该值又产生新的参数值。

   通常，可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已**收敛**。

关键字词：

[收敛](https://developers.google.cn/machine-learning/crash-course/glossary#convergence)、[损失](https://developers.google.cn/machine-learning/glossary#loss)、[训练](https://developers.google.cn/machine-learning/glossary#training)

## 梯度下降法

## 学习速率

梯度下降法算法用梯度乘以一个称为**学习速率**（有时也称为**步长**）的标量，以确定下一个点的位置。

- 如果您选择的学习速率过小，就会花费太长的学习时间；
- 相反，如果您指定的学习速率过大，下一个点将永远在 U 形曲线的底部随意弹跳；
- 每个回归问题都存在一个[金发姑娘](https://wikipedia.org/wiki/Goldilocks_principle)学习速率。“金发姑娘”值与损失函数的平坦程度相关。如果您知道损失函数的梯度较小，则可以放心地试着采用更大的学习速率，以补偿较小的梯度并获得更大的步长。

关键字词：

[超参数](https://developers.google.cn/machine-learning/crash-course/glossary#hyperparameter)、[学习速率](https://developers.google.cn/machine-learning/crash-course/glossary#learning_rate)、[步长](https://developers.google.cn/machine-learning/crash-course/glossary#step_size)

## 随机梯度下降

在梯度下降法中，**批量**指的是用于在单次迭代中计算梯度的样本总数。到目前为止，我们一直假定批量是指整个数据集。就 Google 的规模而言，数据集通常包含数十亿甚至数千亿个样本。此外，Google 数据集通常包含海量特征。因此，一个批量可能相当巨大。如果是超大批量，则单次迭代就可能要花费很长时间进行计算。

包含随机抽样样本的大型数据集可能包含冗余数据。实际上，批量大小越大，出现冗余的可能性就越高。一些冗余可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大型批量高。

如果我们可以通过更少的计算量得出正确的平均梯度，会怎么样？通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱）出较大的平均值。 **随机梯度下降法**(**SGD**) 将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。

**小批量随机梯度下降法**（**小批量 SGD**）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。

为了简化说明，我们只针对单个特征重点介绍了梯度下降法。请放心，梯度下降法也适用于包含多个特征的特征集。