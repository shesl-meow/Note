# 基础知识准备

## 基础概念

> 因为笔者太垃圾，补充的一些譬如《线性代数》、《概率论与数理统计》等的知识

正定矩阵：

- 

熵（Entropy）：

- 热力学中的概念，由香农引入到信息论中；
- 设 $$X \in \{x_1, x_2, \cdots, x_n\}$$ 为一个离散随机变量，其概率分布为 $$p(X = x_i) = p_i, i = 1,2,\cdots, n$$，则 X 的熵为： $$\displaystyle H(X) = - \sum_{i = 1}^n p_i log p_i, \text{ where when pi = 0, define } H(X) = 0$$

极大似然估计：

- 

先验概率、后验概率：

- 

边缘概率：

- 

马尔可夫链蒙特卡洛（`Markov chain Monte Carlo`，`MCMC`）方法：

- 

## 信息检索

倒排索引（inverted index）：

- 现代搜索引擎的核心技术之一，其核心目标是：用 O(1) 的时间复杂度将从大量文档中查找包含某些词的文档集合；
- 这里不会对这一技术深入介绍，可以深入学习相关文献或者开源的倒排索引工具 `Lucene`；

向量空间模型（Vector Space Model, VSM）：文档相似度度量方法之一，它有两个核心技术：

1. 文档表示方式用”词袋假设“（Bags of words, BoW）：
   - 用各个关键词在文档中的“强度”组成的向量表示这个文档：$$d = (x_1, x_2, \cdots, x_M)^T$$
   - 其中“强度”一般用 `TF-IDF` 方式计算，即 $$TF * IDF$$。
   - TF 为该词在文档中出现的次数；
   - IDF 是倒数文档频率，表示一个词在所有文档中出现的频繁程度，最常见的计算形式为 $$\displaystyle IDF(m) = log(\frac{N}{DF(m)})$$，其中 DF 为这个出现了这个词的文档总个数；
2. 计算两个文档的相似度一般使用矢量的余弦距离：$$\displaystyle cos(d_1, d_2) = \frac{d_1^T d_2}{|d_1||d_2|}$$

虽然 VSM 不是实际系统中使用的方法，不过这是一种简单、无训练的基线方法，在探索更精细模型时，必须首先与 VSM 做比较。

## 最优化方法

最优化问题：

- 给定某个确定的目标函数 f，以及该函数自变量满足的一些约束条件，求解该函数的最大值或最小值问题；
- 可以一般地表示为：求 $$min f(x)$$，使得 $$g(x) \le 0, h(x) = 0,$$

### 拉格朗日乘子法

> 参考：https://www.cxybb.com/article/Emma_Love/85146800

目标：

- 用于解决带约束条件下的优化问题；

基本思想：

- 通过引入拉格朗日乘子，将含有 `n` 个变量、`k` 个约束条件**的**约束优化问题转化为含有`(n+k)` 个变量的无约束优化问题。

具体实现：

- 将一般的带约束优化问题：$$\min{f(\vec{x})}, \text{s.t.} g(\vec{x}) \le 0, h(\vec{x}) = 0$$
- 转化为如下不带约束的拉格朗日对偶函数（Lagrange Dual Function）：$$L(\vec{\lambda}, \vec{v}) = \inf_x{f(\vec{x}) + \vec{\lambda}^Tg(\vec{x}) + \vec{v}^T h(\vec{x})}$$，其中 Inf 为下确界函数、$$\vec{\lambda}, \vec{v}$$ 为拉格朗日乘子。
- 可以证明，对偶问题的最优值为原问题最优值的下界，两者完全一致时称为“强对偶”；可以证明，原问题时凸优化问题时，各个约束函数可行域的解也是凸的话，强对偶总是被满足的。

KKT 条件，设最优解是 $$\vec{x}^*$$

- $$x^*$$ 要原题设中的约束条件：$$g(\vec{x^*}) \le 0, h(\vec{x^*}) = 0$$
- $$\vec{\lambda} > 0$$：对不等式进行分情况讨论后得出的必要条件；
- $$\lambda_i \cdot g(x_i^*) = 0$$：称作松弛互补条件；

### 下降单纯形法

在自变量为一维的情况下，给定一个初始区间，假设区间内有唯一的最小值，可以按照分割的方法不断缩小区间以得到最小值；

上面的方法推广到高维的情形对应的算法被称为“下降单纯形法”（Downhill Simplex Method），或者将阿米巴（Ameba）变形虫法。

### 梯度下降法

当目标函数 f 比较容易进行求导时，基于梯度的方法是首要选择，就是每次沿着梯度相反的方向前进一小步，迭代得到最优解，这种方法称为“梯度下降法”（Gradient Descent），其更新公式为：

- $$x \leftarrow x - \epsilon \cdot \nabla{f(x)}$$，其中 $$\epsilon$$ 控制着下降速度，被称为学习率；

梯度下降的另一种变形是“随机梯度下降法”（Stochastic Gradient Descent, SGD）：

- 更新公式：$$x \leftarrow x - \epsilon \cdot \nabla{f^i(x)}$$，区别在于其中 $$\nabla{f^i(x)}$$ 是基于少量样本构成的样本组计算出的梯度；
- 解决的问题：SGD 相对于 GD 解决了收敛速度慢和陷入局部最优的问题；

### 拟牛顿法



## 统计机器学习

